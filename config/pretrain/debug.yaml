setting:
  seed: 20000812
  os_environ:
    WANDB_API_KEY: ~
    WANDB_RUN_ID: ~
    CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
    MASTER_ADDR: 127.0.0.1
    MASTER_PORT: 12316
    WORLD_SIZE: 1
    NODE_RANK: 0
  tensorboard:
    log_dir: tmp/
    name: debug

model:
#    Which model to use
  model_py_path: esm_mamba/esm_mamba_lm_model
  kwargs:
#    Arguments to initialize the specific class
    config_path: config/model/esm_mamba_8M.yaml
    load_pretrained: False
    extra_config:
      gradient_checkpointing: False

#    Arguments to initialize the basic class AbstractModel
  lr_scheduler_kwargs:
    last_epoch: -1
    init_lr: 0
    max_lr: 4.0e-4
    final_lr: 4.0e-5
    warmup_steps: 2000
    start_decay_after_n_steps: 150000
    end_decay_after_n_steps: 1500000
#    Weather to use this scheduler or not
    on_use: True

  optimizer_kwargs:
    betas: [0.9, 0.98]
    weight_decay: 0.01

  save_path: results/debug.pt
  load_prev_scheduler: false
  save_weights_only: false


dataset:
#    Arguments to initialize the basic class LMDBDataset
  dataset_py_path: esm/esm_lm_dataset
  dataloader_kwargs:
    batch_size: 1024
    num_workers: 32

  train_lmdb: /cto_labs/AIDD/DATA/uniref/uniref50/lmdb/train_dedup/data.lmdb
  valid_lmdb: /cto_labs/AIDD/DATA/uniref/uniref50/lmdb/valid/data.lmdb
  test_lmdb: /cto_labs/AIDD/DATA/uniref/uniref50/lmdb/valid/data.lmdb
#    Arguments to initialize the specific class
  kwargs:
#    Use ESM2 tokenizer for now
    tokenizer: /cto_labs/AIDD/WEIGHTS/Protein/esm2_t6_8M_UR50D
    max_length: 1024
    mask_ratio: 0.15


#  Arguments to initialize Pytorch Lightning Trainer
Trainer:
#  max_epochs: 100
  max_steps: 1000000
  min_steps: 1000000
  log_every_n_steps: 100
  accelerator: gpu
  devices: 1
  accumulate_grad_batches: 1
  num_nodes: 1
  strategy:
    find_unused_parameters: True
    static_graph: True
  logger: True
  enable_checkpointing: false
  num_sanity_val_steps: 5
  val_check_interval: 5000
  precision: 16
  limit_val_batches: 0.1
  limit_test_batches: 0.01
