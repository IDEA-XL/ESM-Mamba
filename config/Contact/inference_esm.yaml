setting:
  seed: 20000812
  os_environ:
    WANDB_API_KEY: ~
    WANDB_RUN_ID: ~
    CUDA_VISIBLE_DEVICES: 0,1,2,3
    MASTER_ADDR: localhost
    MASTER_PORT: 12316

    WORLD_SIZE: 1
    NODE_RANK: 0
  tensorboard:
    log_dir: tmp/Contact
    name: original_esm_8M

model:
#    Which model to use
  model_py_path: esm/esm_contact_model_hf
  kwargs:
#    Arguments to initialize the specific class
    config_path: /cto_labs/AIDD/WEIGHTS/Protein/esm2_t33_650M_UR50D
    load_pretrained: True
#    Whether to freeze backbone
    freeze_backbone: True

#    Arguments to initialize the basic class AbstractModel
  lr_scheduler_kwargs:
    last_epoch: -1
    init_lr: 2.0e-3
#    Weather to use this scheduler or not
    on_use: false

  optimizer_kwargs:
    betas: [0.9, 0.98]
    weight_decay: 0.01

  save_path: ~


dataset:
#    Arguments to initialize the basic class LMDBDataset
  dataset_py_path: esm/esm_contact_dataset
  dataloader_kwargs:
    batch_size: 4
    num_workers: 0

  train_lmdb: /cto_labs/AIDD/DATA/SaProt/proteinnet/train
  valid_lmdb: /cto_labs/AIDD/DATA/SaProt/proteinnet/valid
  test_lmdb: /cto_labs/AIDD/DATA/SaProt/proteinnet/valid
#    Arguments to initialize the specific class
  kwargs:
    tokenizer: /cto_labs/AIDD/WEIGHTS/Protein/esm2_t6_8M_UR50D


#  Arguments to initialize Pytorch Lightning Trainer
Trainer:
  max_epochs: 200
  log_every_n_steps: 1
  strategy:
    find_unused_parameters: false
  logger: false
  enable_checkpointing: false
  val_check_interval: 0.5
  accelerator: gpu
  devices: 1
  num_nodes: 1
  accumulate_grad_batches: 1
  precision: 32
  num_sanity_val_steps: 2
  limit_train_batches: 0.0

